# English-to-Turkish-Translation-Model

# Objective
The objective of this project is to fine-tune a pre-trained translation model to translate text from English to Turkish. The fine-tuned model is evaluated using both BLEU and SacreBLEU metrics on the [WMT16 dataset](https://huggingface.co/datasets/wmt16) for English-Turkish translation.

# File Structure
- `TranslationEtoT.ipynb`: Runnable Colab notebook. This notebook can be executed on Google Colab with A100 GPU runtime.
- `predictions_baseline.txt`: Predictions on the test set generated by the baseline model.
- `predictions_finetuned_2.txt`: Predictions on the test set generated by the fine-tuned model.

# Usage
## Baseline Model Evaluation

To evaluate the [baseline model](https://huggingface.co/Helsinki-NLP/opus-tatoeba-en-tr) on the test set, the following steps are used:

1. Change the source language and target language to English and Turkish respectively.
2. Use the provided baseline model for English-Turkish translation.
3. Utilize the WMT16 dataset for testing.
4. Evaluate the baseline model using both BLEU and SacreBLEU metrics.

## Fine-tuning and Evaluation

To fine-tune and evaluate the model, the following steps are used:

1. Fine-tune the baseline model with a label smoothing factor of 0.1 and utilize half-precision training (fp16).
2. Evaluate the fine-tuned model on the test set using both BLEU and SacreBLEU metrics.

# Results

The results of both the baseline model evaluation and the fine-tuned model evaluation are provided in the repository.
|  | BLEU | SacreBLEU |
|----------|----------|----------|
| Baseline Model | 0.178 | 21.4 |
| Fine-tuned Model | 0.166 | 20.3 |


# Dependencies
- PyTorch
- Hugging Face Transformers
